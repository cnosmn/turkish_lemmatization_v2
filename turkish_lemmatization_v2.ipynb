{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2df457d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\osman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from fuzzywuzzy import process\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import zeyrek   #veya   SnowballStemmer kullanılabilir (daha test edilmedi)\n",
    "analyzer = zeyrek.MorphAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f965987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install SnowballStemmer\n",
    "#!pip install stop_words\n",
    "#!pip install zeyrek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ffa9a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kanun_lemma(kanun_cumleler):\n",
    "    \n",
    "    lemmatization = []   \n",
    "    kanun_madde_eski = {}\n",
    "    \n",
    "    for i in kanun_cumleler.keys():\n",
    "\n",
    "        i_1 = word_tokenize(i) #splitted\n",
    "        i_1 = str(i_1)\n",
    "\n",
    "        #kanunlar tokenlerine ayrıldı : \n",
    "\n",
    "        #noktalama işaretleri silme\n",
    "        \n",
    "        # initializing punctuations string\n",
    "        punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "        for ele in i_1:\n",
    "            if ele in punc:\n",
    "                i_1 = i_1.replace(ele, \"\")\n",
    "\n",
    "        #köklerine ayırma        \n",
    "        ad1 = []\n",
    "        ad1.append(i_1)\n",
    "\n",
    "        tune1 = []\n",
    "\n",
    "        for x in ad1:\n",
    "            for i in analyzer.lemmatize(x):\n",
    "                tune1.append(i[1])\n",
    "\n",
    "        con1=[]\n",
    "\n",
    "        for j in tune1:\n",
    "            step1 = 0\n",
    "            con1.append(j[step1])\n",
    "            step1= step1 + 1\n",
    "\n",
    "        con1_1 = str(con1)\n",
    "\n",
    "\n",
    "        #gereksiz kelimeler silindi\n",
    "\n",
    "        nltk.download('stopwords')\n",
    "        stopWords = set(stopwords.words('turkish'))\n",
    "\n",
    "        wordsFiltered1 = []\n",
    "\n",
    "        for w in con1:\n",
    "            if w not in stopWords:\n",
    "                wordsFiltered1.append(w)\n",
    "\n",
    "        \n",
    "        wordsFiltered1_1 = str(wordsFiltered1)\n",
    "        #filtre edilmiş cümle \n",
    "\n",
    "        bos1= \" \"\n",
    "        for n in wordsFiltered1:\n",
    "            bos1= bos1+\" \"+ n\n",
    "        \n",
    "        lemmatization.append(bos1)\n",
    "        \n",
    "    kanun_values = []\n",
    "    for i in kanun_cumleler.values():\n",
    "        kanun_values.append(i)\n",
    "    \n",
    "    for k in range(len(lemmatization)):\n",
    "        kanun_madde_eski.setdefault(lemmatization[k],kanun_values[k])\n",
    "        \n",
    "    return lemmatization , kanun_madde_eski\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc64f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(cumle1):\n",
    "\n",
    "    #noktalama işaretleri silindi\n",
    "    \n",
    "    cumle1_1 = word_tokenize(cumle1) #splitted\n",
    "\n",
    "    cumle1_1 = str(cumle1_1)\n",
    "    \n",
    "    print(\"cümle 1 tokenlerine ayrıldı : \" + cumle1_1)\n",
    "    print(\"  \")\n",
    "\n",
    "    \n",
    "\n",
    "    print('cümle 1 : '+ cumle1_1)\n",
    "    print(\"  \")\n",
    "\n",
    "\n",
    "    #noktalama işaretleri silme\n",
    "    \n",
    "    # printing original string\n",
    "    print(\"The original string is : \" + cumle1_1)\n",
    "    print(\"  \")\n",
    "    \n",
    "    # initializing punctuations string\n",
    "    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "    for ele in cumle1_1:\n",
    "        if ele in punc:\n",
    "            cumle1_1 = cumle1_1.replace(ele, \"\")\n",
    "            \n",
    "\n",
    "    # printing result\n",
    "    print(\"The string after punctuation filter : \" +  cumle1_1)\n",
    "    print(\"  \")\n",
    "\n",
    "    \n",
    "    ad1 = []\n",
    "    ad1.append(cumle1_1)\n",
    "    \n",
    "    \n",
    "    tune1 = []\n",
    " \n",
    "    for x in ad1:\n",
    "        for i in analyzer.lemmatize(x):\n",
    "            tune1.append(i[1])\n",
    "        \n",
    "    \n",
    "    con1=[]\n",
    "\n",
    "    for i in tune1:\n",
    "        step1 = 0\n",
    "        con1.append(i[step1])\n",
    "        step1= step1 + 1\n",
    "  \n",
    "    con1_1 = str(con1)\n",
    "    \n",
    "    print(\"köklerine ayrılmış cümle 1 :\" + con1_1)\n",
    "    print(\"  \")\n",
    "\n",
    "    #gereksiz kelimeler silindi\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    stopWords = set(stopwords.words('turkish'))\n",
    "    \n",
    "    wordsFiltered1 = []\n",
    "    \n",
    "    for w in con1:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered1.append(w)\n",
    "\n",
    "    \n",
    "    wordsFiltered1_1 = str(wordsFiltered1)\n",
    "    \n",
    "    print(\"filtre edilmiş cümle 1 :\" + wordsFiltered1_1)\n",
    "    print(\"  \")\n",
    "    \n",
    "    '''\n",
    "    temp = []\n",
    "    \n",
    "    for i in wordsFiltered1:\n",
    "        print(\"{} kelimesinin {}  kelimelerine benzerlik  oranları \".format(i,wordsFiltered2))\n",
    "        print(process.extract(i, wordsFiltered2))\n",
    "        print(\"  \")\n",
    "        temp.append(process.extract(i, wordsFiltered2))\n",
    "         \n",
    "\n",
    "    sum = 0\n",
    "    ort = 0\n",
    "    sayac = 0\n",
    "    for i in temp:\n",
    "        for j in i:\n",
    "            sayac = sayac + 1\n",
    "            sum = sum + j[1]\n",
    "    skor = int(sum/sayac)\n",
    "    print(\"kelimenin skoru : {}\".format(skor))\n",
    "    \n",
    "    '''  \n",
    "    bos1= \" \"\n",
    "    for n in wordsFiltered1:\n",
    "        bos1= bos1+\" \"+ n\n",
    "    \n",
    "    from fuzzywuzzy import process  \n",
    "    lemmatization , kanun_madde_eski = kanun_lemma(kanun_cumleler)\n",
    "    print(\"{} cümlesinin benzerlik oranları : \".format(bos1))\n",
    "    print(process.extract(bos1,lemmatization))\n",
    "    print(\" \")\n",
    "    \n",
    "    score = process.extract(bos1,lemmatization)\n",
    "    best = 0\n",
    "\n",
    "    #en yüksek benzerlikli degeri bulma\n",
    "   \n",
    "    for i in range(len(score)):\n",
    "        for j in range(1,2):\n",
    "            if best <score[i][j]:\n",
    "                best = score[i][j]\n",
    "                \n",
    "    for l in range(len(score)):\n",
    "        for p in range(1,2):\n",
    "            if best == score[l][p]:\n",
    "                param = score[l][0]\n",
    "                \n",
    "    print(param)\n",
    "    print(\" \")\n",
    "    print(\"en yüksek benzerlikli madde : \"+kanun_madde_eski.get(param, \"Madde Bulunamadı!\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b321bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kanun_cumleler = {\"Taksirle başkasının vücuduna acı veren veya sağlığının ya da algılama yeteneğinin bozulmasına nedenolan kişi, üç aydan bir yıla kadar hapis veya adlî para cezası ile cezalandırılır.\":\"madde 89\",\n",
    "\"Kasten başkasının vücuduna acı veren veya sağlığının ya da algılama yeteneğinin bozulmasına neden olan kişi, bir yıldan üç yıla kadar hapis cezası ile cezalandırılır\": \"madde 86\"}  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b8a090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cümle 1 tokenlerine ayrıldı : ['kasten', 'birinin', 'vücuduna', 'zarar', 'vermek', '.']\n",
      "  \n",
      "cümle 1 : ['kasten', 'birinin', 'vücuduna', 'zarar', 'vermek', '.']\n",
      "  \n",
      "The original string is : ['kasten', 'birinin', 'vücuduna', 'zarar', 'vermek', '.']\n",
      "  \n",
      "The string after punctuation filter : kasten birinin vücuduna zarar vermek \n",
      "  \n",
      "köklerine ayrılmış cümle 1 :['kasten', 'bir', 'vücut', 'zarar', 'vermek']\n",
      "  \n",
      "filtre edilmiş cümle 1 :['kasten', 'bir', 'vücut', 'zarar', 'vermek']\n",
      "  \n",
      "  kasten bir vücut zarar vermek cümlesinin benzerlik oranları : \n",
      "[('  taksir başka vücut acımak vere sağ algılamak yetenek bozmak nedenolan kişi üç ay bir yılmak kadar hapis ad par ceza cezalandırmak', 86), ('  kasten başka vücut acımak vere sağ algılamak yetenek bozmak kişi bir yıl üç yılmak kadar hapis ceza cezalandırmak', 86)]\n",
      " \n",
      "  kasten başka vücut acımak vere sağ algılamak yetenek bozmak kişi bir yıl üç yılmak kadar hapis ceza cezalandırmak\n",
      " \n",
      "en yüksek benzerlikli madde : madde 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\osman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\osman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\osman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "lemma('kasten birinin vücuduna zarar vermek.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b101da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
